import pandas as pd
import numpy as np
import xgboost as xgb
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import os

class NoiseClassifierXGBoost:
    def __init__(self, dataset_csv_path, model_save_dir="./noise_classifier_model"):
        """
        Args:
            dataset_csv_path: Path to the dataset CSV generated by NoiseDatasetGenerator
            model_save_dir: Directory to save the trained model
        """
        self.dataset_csv_path = dataset_csv_path
        self.model_save_dir = model_save_dir
        self.model = None
        self.label_encoder = None
        self.scaler = None
        self.feature_names = None
        
        Path(model_save_dir).mkdir(parents=True, exist_ok=True)
        print(f"‚úì Model directory: {model_save_dir}")
    
    def load_and_prepare_data(self, test_size=0.2, random_state=42):
        """Load CSV and prepare data for training"""
        print("\nüìÇ Loading dataset...")
        df = pd.read_csv(self.dataset_csv_path)
        
        print(f"‚úì Dataset loaded: {df.shape[0]} samples, {df.shape[1]} features")
        print(f"\nClass distribution:")
        print(df['noise_type'].value_counts())
        
        # Define feature columns (exclude image name, noise type, clean image, intensity)
        exclude_cols = ['image_name', 'noise_type', 'clean_image', 'intensity']
        self.feature_names = [col for col in df.columns if col not in exclude_cols]
        
        print(f"\nüìä Using {len(self.feature_names)} features for training:")
        print(f"   {', '.join(self.feature_names)}")
        
        X = df[self.feature_names].values
        y = df['noise_type'].values
        
        # Encode labels
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y)
        
        print(f"\nClass mapping:")
        for i, class_name in enumerate(self.label_encoder.classes_):
            print(f"   {i}: {class_name}")
        
        # Scale features
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_encoded, test_size=test_size, random_state=random_state,
            stratify=y_encoded  # Ensure balanced splits
        )
        
        print(f"\n‚úì Data prepared:")
        print(f"   Training samples: {X_train.shape[0]}")
        print(f"   Test samples: {X_test.shape[0]}")
        
        return X_train, X_test, y_train, y_test
    
    def train(self, X_train, y_train, **xgb_params):
        """Train XGBoost classifier"""
        print("\nüöÄ Training XGBoost model...")
        
        # Default parameters
        default_params = {
            'objective': 'multi:softmax',
            'num_class': len(self.label_encoder.classes_),
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 200,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'eval_metric': 'mlogloss',
            'random_state': 42,
            'verbosity': 1,
        }
        default_params.update(xgb_params)
        
        # Pass n_estimators in the constructor (don't use a non-existent 'epochs' arg)
        self.model = xgb.XGBClassifier(**default_params)
        self.model.fit(X_train, y_train, verbose=False)

        
        print(f"‚úÖ Model training complete!")
    
    def evaluate(self, X_test, y_test):
        """Evaluate model on test set"""
        print("\nüìà Evaluating model...")
        
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"\n‚úì Accuracy: {accuracy:.4f}")
        
        # Classification report
        print("\nClassification Report:")
        print(classification_report(
            y_test, y_pred,
            target_names=self.label_encoder.classes_
        ))
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        print("\nConfusion Matrix:")
        print(cm)
        
        # Plot confusion matrix
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=self.label_encoder.classes_,
                    yticklabels=self.label_encoder.classes_)
        plt.title('Confusion Matrix - Noise Type Classification')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig(os.path.join(self.model_save_dir, 'confusion_matrix.png'), dpi=150)
        print(f"\n‚úì Confusion matrix saved to {self.model_save_dir}/confusion_matrix.png")
        plt.close()
        
        return accuracy
    
    def plot_feature_importance(self, top_n=15):
        """Plot feature importance"""
        print(f"\nüìä Top {top_n} Important Features:")
        
        importances = self.model.feature_importances_
        indices = np.argsort(importances)[-top_n:][::-1]
        
        top_features = [self.feature_names[i] for i in indices]
        top_importances = importances[indices]
        
        for feat, imp in zip(top_features, top_importances):
            print(f"   {feat}: {imp:.4f}")
        
        # Plot
        plt.figure(figsize=(10, 6))
        plt.barh(range(len(top_importances)), top_importances)
        plt.yticks(range(len(top_importances)), top_features)
        plt.xlabel('Feature Importance')
        plt.title(f'Top {top_n} Important Features for Noise Classification')
        plt.tight_layout()
        plt.savefig(os.path.join(self.model_save_dir, 'feature_importance.png'), dpi=150)
        print(f"\n‚úì Feature importance plot saved")
        plt.close()
    
    def save_model(self):
        """Save trained model and components"""
        print(f"\nüíæ Saving model to {self.model_save_dir}...")
        
        # Save model
        model_path = os.path.join(self.model_save_dir, 'xgboost_noise_classifier.pkl')
        self.model.save_model(model_path)
        
        # Save label encoder
        le_path = os.path.join(self.model_save_dir, 'label_encoder.pkl')
        with open(le_path, 'wb') as f:
            pickle.dump(self.label_encoder, f)
        
        # Save scaler
        scaler_path = os.path.join(self.model_save_dir, 'scaler.pkl')
        with open(scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # Save feature names
        features_path = os.path.join(self.model_save_dir, 'feature_names.pkl')
        with open(features_path, 'wb') as f:
            pickle.dump(self.feature_names, f)
        
        print(f"‚úÖ Model saved successfully!")
        print(f"   - Model: {model_path}")
        print(f"   - Label Encoder: {le_path}")
        print(f"   - Scaler: {scaler_path}")
        print(f"   - Features: {features_path}")
    
    def predict_from_image(self, image_path):
        """Load an image and predict its noise type"""
        import cv2
        
        if self.model is None:
            raise ValueError("Model not trained yet!")
        
        print(f"\nüîç Predicting noise type for: {image_path}")
        
        # Read image
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Could not read image: {image_path}")
        
        # Extract features (same as training)
        from Prepare_Folder import NoiseDatasetGenerator
        gen = NoiseDatasetGenerator(".", ".")
        features = gen.extract_features(image)
        
        # Order features correctly
        feature_vector = np.array([features[fname] for fname in self.feature_names]).reshape(1, -1)
        
        # Scale
        feature_vector_scaled = self.scaler.transform(feature_vector)
        
        # Predict
        pred_encoded = self.model.predict(feature_vector_scaled)[0]
        pred_label = self.label_encoder.inverse_transform([int(pred_encoded)])[0]
        
        # Get probabilities
        pred_proba = self.model.predict_proba(feature_vector_scaled)[0]
        
        print(f"\n‚úì Predicted noise type: {pred_label}")
        print(f"\nProbabilities:")
        for cls, prob in zip(self.label_encoder.classes_, pred_proba):
            print(f"   {cls}: {prob:.4f}")
        
        return pred_label, pred_proba


# ============ USAGE ============
import os

if __name__ == "__main__":
    # Path to dataset CSV
    DATASET_CSV = "./noise_dataset/dataset.csv"
    MODEL_SAVE_DIR = "./noise_classifier_model"
    
    # Initialize classifier
    classifier = NoiseClassifierXGBoost(DATASET_CSV, MODEL_SAVE_DIR)
    
    # Load and prepare data
    X_train, X_test, y_train, y_test = classifier.load_and_prepare_data(test_size=0.2)
    
    # Train model
    classifier.train(X_train, y_train, 
                    max_depth=7, 
                    learning_rate=0.1, 
                    n_estimators=200)
    
    # Evaluate
    accuracy = classifier.evaluate(X_test, y_test)
    
    # Plot feature importance
    classifier.plot_feature_importance(top_n=15)
    
    # Save model
    classifier.save_model()
    
    print(f"\nüéâ Training complete!")
    print(f"Model saved to: {MODEL_SAVE_DIR}")
    print(f"Final Accuracy: {accuracy:.4f}")